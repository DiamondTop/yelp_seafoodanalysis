{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import API key\n",
    "from api_keys import api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Authorization': 'Bearer %s' % api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search link on yelp\n",
    "url='https://api.yelp.com/v3/businesses/search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running search through yelp and return the max results\n",
    "data = []\n",
    "count = 0\n",
    "for offset in range(0, 1000, 50):\n",
    "        params = {\n",
    "            'limit': 50, \n",
    "            'location':'New York',\n",
    "            'term': 'seabass',\n",
    "            'offset': offset\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data += response.json()['businesses']\n",
    "            count = count + 1\n",
    "        elif response.status_code == 400:\n",
    "            print('400 Bad Request')\n",
    "            break\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[998]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the results of the search in dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['is_closed', 'transactions', 'distance', 'coordinates', 'phone', 'image_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "df.location[1][\"display_address\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.categories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.categories[0][0][\"alias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.categories[0][0][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.categories[1][0][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.categories[2][0][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop null data values- price seems to be missing\n",
    "#---\n",
    "df.drop('price', axis=1)\n",
    "df = df.dropna(axis=0)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split link\n",
    "df[['new_url2', 'new_url3']]=df['url'].str.split('?', expand=True,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_url2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_url2'] = df['new_url2'] + '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dummy new column\n",
    "df[\"New_Category\"] = df.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dummy new column\n",
    "df[\"New_zip_code\"] = df.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dummy new column\n",
    "df[\"new_ID\"] = df[\"id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dummy new column\n",
    "df[\"URL\"] = df[\"new_url2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dummy new column\n",
    "df[\"Updated_city\"] = df.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location[1][\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location[1][\"zip_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new dummy column for city, zip code, and food category\n",
    "counting = 0\n",
    "for k in range(0,len(df)):\n",
    "    df[\"New_Category\"][k] = df.categories[k][0][\"title\"]\n",
    "    df[\"Updated_city\"][k] = df.location[k][\"city\"]\n",
    "    df[\"New_zip_code\"][k] = df.location[k][\"zip_code\"] \n",
    "    #df[\"URL\"][k] = df['URL'][k] + \"?\"\n",
    "    #df[\"URL\"][k] = df[\"new_url2\"][k] + \"?\"\n",
    "    #print(\"No:\" + str(counting))\n",
    "    #print(\"URL:\", df.URL[k])\n",
    "    #print(\"\\n\")\n",
    "    #counting = counting + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Updated_city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"New_Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create new table under df_graph\n",
    "\n",
    "df_graph = df[[\"name\", \"new_ID\",\"URL\",\"New_Category\", \"price\", \"rating\",\n",
    "                           \"Updated_city\", \"review_count\",\"location\",\"New_zip_code\"]]\n",
    "\n",
    "df_graph.index.name = 'Index'\n",
    "df_graph.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = us_restaurants.city.value_counts()[:10]\n",
    "\n",
    "\n",
    "new_cat = df_graph[\"New_Category\"].value_counts()[:10]\n",
    "new_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_group = df_graph.groupby('rating')\n",
    "rat_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_cat_group = rat_group['rating'].count()\n",
    "rat_cat_group.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_chart = rat_cat_group.plot(kind='barh', figsize=(8, 10), color='#86bf91', zorder=-1, width=0.5, title='Ratings View')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_group = df_graph.groupby('New_Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cat_group = cat_group['New_Category'].count().sort_values(ascending = False)[:10]\n",
    "count_cat_group.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_chart= new_cat.plot(kind='barh')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "grouped = df_graph.groupby('New_Category')['review_count'].sum().sort_values(ascending = False)[:10]\n",
    "sns.barplot(y=grouped.index, x= grouped.values, palette= sns.color_palette(\"RdBu_r\", len(grouped)) )\n",
    "plt.ylabel('Category', fontsize=14)\n",
    "plt.xlabel('Count of reviews', fontsize=14)\n",
    "plt.title('Count of Reviews by Cuisine Type for 1000 restaurants', fontsize=15)\n",
    "for i,v in enumerate(grouped):\n",
    "    plt.text(v, i+0.15, str(v),fontweight='bold', fontsize=14)\n",
    "plt.tick_params(labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_chart = new_cat.plot(kind='barh', figsize=(8, 10), color='#86bf91', zorder=-1, width=0.5, title='Top 10 categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart based off of the group series from before\n",
    "count_chart = new_cat.plot(kind='bar', title='Top 10 categories')\n",
    "\n",
    "# Set the xlabel and ylabel using class methods\n",
    "count_chart.set_xlabel(\"Number of restaurants\")\n",
    "count_chart.set_ylabel(\"Count of Restaurants by Category\")\n",
    "for index,data in enumerate(new_cat):\n",
    "    plt.text(x=index, y =data+4, s=f\"{data}\", horizontalalignment ='center',color='green', fontdict=dict(fontsize=14), va='center')\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP 10 Restaurants with Most Reviews\n",
    "plt.figure(figsize=(11,6))\n",
    "grouped = df_graph[['name','review_count']].sort_values(by='review_count', ascending=False)[:10]\n",
    "sns.barplot(x=grouped.review_count, y = grouped.name, palette=sns.color_palette(\"GnBu_r\", len(grouped)), ci=None)\n",
    "plt.xlabel('Count of Review', labelpad=10, fontsize=14)\n",
    "plt.ylabel('Restaurants', fontsize=14)\n",
    "plt.title('TOP 10 NYC Restaurants with Most Reviews for seabass', fontsize=15)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xticks(rotation=15)\n",
    "for  i, v in enumerate(grouped.review_count):\n",
    "    plt.text(v, i, str(v), fontweight='bold', color='blue', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,6))\n",
    "\n",
    "\n",
    "grouped = df_graph.groupby('New_zip_code')['review_count'].sum().sort_values(ascending=False)[:10]\n",
    "grouped_df = pd.DataFrame(grouped.reset_index())\n",
    "print(grouped_df)\n",
    "grouped_df.columns =[\"New_zip_code\",\"review_count\"]\n",
    "\n",
    "grouped_df = grouped_df.sort_values(['review_count'],ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "ax = sns.barplot(grouped_df.index, grouped_df.review_count,palette=sns.color_palette(\"GnBu_r\", len(grouped_df))) \n",
    "plt.xlabel('Zipcode', labelpad=10, fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.title('Count of Reviews by Zip Code in NYC (Top 10)', fontsize=15)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "for  i, v in enumerate(grouped):\n",
    "    plt.text(i, v*1.02, str(v), horizontalalignment ='center',color='blue',rotation=45,fontweight='bold', fontsize=14)\n",
    "    \n",
    "\n",
    "ax.set_xticklabels(grouped_df.New_zip_code)\n",
    "for item in ax.get_xticklabels(): item.set_rotation(45)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label reviews as positive or negative\n",
    "df['labels'] = ''\n",
    "df.loc[df.rating >=4, 'labels'] = 'positive'\n",
    "df.loc[df.rating ==3, 'labels'] = 'neural'\n",
    "df.loc[df.rating <3, 'labels'] = 'negative'\n",
    "\n",
    "# drop neutral reviews for easy analysis\n",
    "df.drop(df[df['labels'] =='neural'].index, axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the earliest/latest year a book was published\n",
    "least_rev = df_graph[\"review_count\"].min()\n",
    "most_rev = df_graph[\"review_count\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_reviews = df_graph.nlargest(5,['review_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 3 most reviews resturants based on search terms'seabass'\n",
    "top_reviews['URL'].iloc[0]\n",
    "top_reviews['URL'].iloc[1]\n",
    "top_reviews['URL'].iloc[2]\n",
    "top_reviews['URL'].iloc[3]\n",
    "top_reviews['URL'].iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 3 recommended resturants based on search terms 'seabass'\n",
    "df_graph['URL'].loc[0] # fish-cheeks\n",
    "df_graph['URL'].loc[1] # kotobuki\n",
    "df_graph['URL'].loc[2] # midtown catch\n",
    "df_graph['URL'].loc[3] # au-zaata\n",
    "df_graph['URL'].loc[4] # thai-villa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph['URL'].loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "#Searching for Top 3 resturants on the word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages, Dependencies\n",
    "from selenium import webdriver\n",
    "from splinter import Browser\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing module for regex \n",
    "import re\n",
    "cList = {\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\",\n",
    "  \"can't've\": \"cannot have\",\n",
    "  \"'cause\": \"because\",\n",
    "  \"could've\": \"could have\",\n",
    "  \"couldn't\": \"could not\",\n",
    "  \"couldn't've\": \"could not have\",\n",
    "  \"didn't\": \"did not\",\n",
    "  \"doesn't\": \"does not\",\n",
    "  \"don't\": \"do not\",\n",
    "  \"hadn't\": \"had not\",\n",
    "  \"hadn't've\": \"had not have\",\n",
    "  \"hasn't\": \"has not\",\n",
    "  \"haven't\": \"have not\",\n",
    "  \"he'd\": \"he would\",\n",
    "  \"he'd've\": \"he would have\",\n",
    "  \"he'll\": \"he will\",\n",
    "  \"he'll've\": \"he will have\",\n",
    "  \"he's\": \"he is\",\n",
    "  \"how'd\": \"how did\",\n",
    "  \"how'd'y\": \"how do you\",\n",
    "  \"how'll\": \"how will\",\n",
    "  \"how's\": \"how is\",\n",
    "  \"I'd\": \"I would\",\n",
    "  \"I'd've\": \"I would have\",\n",
    "  \"I'll\": \"I will\",\n",
    "  \"I'll've\": \"I will have\",\n",
    "  \"I'm\": \"I am\",\n",
    "  \"I've\": \"I have\",\n",
    "  \"isn't\": \"is not\",\n",
    "  \"it'd\": \"it had\",\n",
    "  \"it'd've\": \"it would have\",\n",
    "  \"it'll\": \"it will\",\n",
    "  \"it'll've\": \"it will have\",\n",
    "  \"it's\": \"it is\",\n",
    "  \"let's\": \"let us\",\n",
    "  \"ma'am\": \"madam\",\n",
    "  \"mayn't\": \"may not\",\n",
    "  \"might've\": \"might have\",\n",
    "  \"mightn't\": \"might not\",\n",
    "  \"mightn't've\": \"might not have\",\n",
    "  \"must've\": \"must have\",\n",
    "  \"mustn't\": \"must not\",\n",
    "  \"mustn't've\": \"must not have\",\n",
    "  \"needn't\": \"need not\",\n",
    "  \"needn't've\": \"need not have\",\n",
    "  \"o'clock\": \"of the clock\",\n",
    "  \"oughtn't\": \"ought not\",\n",
    "  \"oughtn't've\": \"ought not have\",\n",
    "  \"shan't\": \"shall not\",\n",
    "  \"sha'n't\": \"shall not\",\n",
    "  \"shan't've\": \"shall not have\",\n",
    "  \"she'd\": \"she would\",\n",
    "  \"she'd've\": \"she would have\",\n",
    "  \"she'll\": \"she will\",\n",
    "  \"she'll've\": \"she will have\",\n",
    "  \"she's\": \"she is\",\n",
    "  \"should've\": \"should have\",\n",
    "  \"shouldn't\": \"should not\",\n",
    "  \"shouldn't've\": \"should not have\",\n",
    "  \"so've\": \"so have\",\n",
    "  \"so's\": \"so is\",\n",
    "  \"that'd\": \"that would\",\n",
    "  \"that'd've\": \"that would have\",\n",
    "  \"that's\": \"that is\",\n",
    "  \"there'd\": \"there had\",\n",
    "  \"there'd've\": \"there would have\",\n",
    "  \"there's\": \"there is\",\n",
    "  \"they'd\": \"they would\",\n",
    "  \"they'd've\": \"they would have\",\n",
    "  \"they'll\": \"they will\",\n",
    "  \"they'll've\": \"they will have\",\n",
    "  \"they're\": \"they are\",\n",
    "  \"they've\": \"they have\",\n",
    "  \"to've\": \"to have\",\n",
    "  \"wasn't\": \"was not\",\n",
    "  \"we'd\": \"we had\",\n",
    "  \"we'd've\": \"we would have\",\n",
    "  \"we'll\": \"we will\",\n",
    "  \"we'll've\": \"we will have\",\n",
    "  \"we're\": \"we are\",\n",
    "  \"we've\": \"we have\",\n",
    "  \"weren't\": \"were not\",\n",
    "  \"what'll\": \"what will\",\n",
    "  \"what'll've\": \"what will have\",\n",
    "  \"what're\": \"what are\",\n",
    "  \"what's\": \"what is\",\n",
    "  \"what've\": \"what have\",\n",
    "  \"when's\": \"when is\",\n",
    "  \"when've\": \"when have\",\n",
    "  \"where'd\": \"where did\",\n",
    "  \"where's\": \"where is\",\n",
    "  \"where've\": \"where have\",\n",
    "  \"who'll\": \"who will\",\n",
    "  \"who'll've\": \"who will have\",\n",
    "  \"who's\": \"who is\",\n",
    "  \"who've\": \"who have\",\n",
    "  \"why's\": \"why is\",\n",
    "  \"why've\": \"why have\",\n",
    "  \"will've\": \"will have\",\n",
    "  \"won't\": \"will not\",\n",
    "  \"won't've\": \"will not have\",\n",
    "  \"would've\": \"would have\",\n",
    "  \"wouldn't\": \"would not\",\n",
    "  \"wouldn't've\": \"would not have\",\n",
    "  \"y'all\": \"you all\",\n",
    "  \"y'alls\": \"you alls\",\n",
    "  \"y'all'd\": \"you all would\",\n",
    "  \"y'all'd've\": \"you all would have\",\n",
    "  \"y'all're\": \"you all are\",\n",
    "  \"y'all've\": \"you all have\",\n",
    "  \"you'd\": \"you had\",\n",
    "  \"you'd've\": \"you would have\",\n",
    "  \"you'll\": \"you you will\",\n",
    "  \"you'll've\": \"you you will have\",\n",
    "  \"you're\": \"you are\",\n",
    "  \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url_top1 = top_reviews['URL'].iloc[0]\n",
    "url_top2 = top_reviews['URL'].iloc[1]\n",
    "url_top3 = top_reviews['URL'].iloc[2]\n",
    "url_top4 = top_reviews['URL'].iloc[3]\n",
    "url_top5 = top_reviews['URL'].iloc[4]\n",
    "\n",
    "url_rec1 = df_graph['URL'].loc[0]\n",
    "url_rec2 = df_graph['URL'].loc[1]\n",
    "url_rec3 = df_graph['URL'].loc[2]\n",
    "url_rec4 = df_graph['URL'].loc[3]\n",
    "url_rec5 = df_graph['URL'].loc[4]\n",
    "\n",
    "url_rec3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve page with the requests module\n",
    "#response = requests.get(url_top1)\n",
    "response = requests.get(url_rec3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seabass\n",
    "#url_top1\n",
    "#url_top2\n",
    "#url_top3\n",
    "#url_rec1\n",
    "#url_rec2\n",
    "url_rec3\n",
    "\n",
    "\n",
    "start = 0\n",
    "num_pages = 7\n",
    "end = 20* num_pages\n",
    "x = 1\n",
    "reviews = []\n",
    "quote_add = []\n",
    "\n",
    "while (start < end):\n",
    "    try:\n",
    "        new_url = url_rec3 + 'start=' + str(start)\n",
    "        start +=20\n",
    "        print (new_url)\n",
    "        response = requests.get(new_url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "    #instructions = recipeSoup.find('div', class_='hidden').text\n",
    "    #right_table4=soup.find_all('p', itemprop='description')\n",
    "        quotes = soup.find_all('span', class_ = 'lemon--span__373c0__3997G raw__373c0__3rKqk')\n",
    "        for quote in quotes:\n",
    "            x = x+1\n",
    "            print('Review:', x, '-------------')\n",
    "            print(quote.text)\n",
    "            quote_add.append(quote.text)\n",
    "#-------------------------------------------------------------------newly added\n",
    "#Makes all the letters lower in reviews\n",
    "            review = quote.text.lower()\n",
    "#expandContractions will put the dictionary made earlier to replace the contractions in the reviews\n",
    "#Make sure to to run the cList dict cell or else there will be an error\n",
    "            expandContractions(review)\n",
    "#Cleaning the lemmas or words in reviews now will make it easier when we start predictive modeling\n",
    "            words = word_tokenize(review)\n",
    "            words = word_tokenize(review.replace('\\n',' '))\n",
    "            clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "            characters_to_remove = [\"''\",'``','...']\n",
    "            clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "            english_stops = set(stopwords.words('english'))\n",
    "            clean_words = [word for word in clean_words if word not in english_stops]\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "            #lemma_list = str.replace(right_table4.get_text(strip=True),\". \",\".\")\n",
    "            reviews.append(lemma_list)\n",
    "    #reviews.add(a)\n",
    "    except:\n",
    "    #reviews.append(right_table4)\n",
    "        time.sleep(5)\n",
    "    #print(\"Reviews:\", a,\"\\n\")\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncleaned\n",
    "reviews_collect = pd.DataFrame(quote_add, columns=['reviews'])\n",
    "reviews_collect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving reviews to csv\n",
    "reviews_collect.to_csv (r'C:\\Users\\leroy\\Desktop\\reviews6.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned\n",
    "reviews_collect_cleaned = pd.DataFrame(np.array(reviews), columns=['review'])\n",
    "reviews_collect_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_name = []\n",
    "y = 0\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for text in quote_add:\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace('/', '')\n",
    "    text = text.replace('?', '')\n",
    "    text = text.replace(')', '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace('!!', '')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace('!', '')\n",
    "    text = text.replace('', '')\n",
    "    text = text.replace(':', '')\n",
    "    text = text.replace(';', '')\n",
    "    #cleaning\n",
    "    #try:\n",
    "          #method = str.replace(text.get_text(strip=True),\". \",\"\")\n",
    "          #method = str.replace(method, \". \", \".\")\n",
    "          #method = str.replace(method, \".\",\".\\n\")\n",
    "          #method = str.replace(method,\", \",\"\") \n",
    "        \n",
    "    print(text)  \n",
    "    #except AttributeError:\n",
    "          #print(\"exit\")\n",
    "    for word in text.split():\n",
    "        if word == \"here\":\n",
    "            word = word.replace('here','')\n",
    "        if word == \"came\":\n",
    "            word = word.replace('came','')\n",
    "        if word == \"would\":\n",
    "            word = word.replace('would','')    \n",
    "        if word == \"go\":\n",
    "            word = word.replace('go','')\n",
    "        if word == \"out\":\n",
    "            word = word.replace('out','')\n",
    "        if word == \"by\":\n",
    "            word = word.replace('by','')    \n",
    "        if word == \"our\":\n",
    "            word = word.replace('our','')\n",
    "        if word == \"from\":\n",
    "            word = word.replace('from','')\n",
    "        if word == \"some\":\n",
    "            word = word.replace('some','')\n",
    "        if word == \"us\":\n",
    "            word = word.replace('us','')\n",
    "        if word == \"or\":\n",
    "            word = word.replace('or','')    \n",
    "        if word == \"can\":\n",
    "            word = word.replace('can','')    \n",
    "        if word == \"all\":\n",
    "            word = word.replace('all','')\n",
    "        if word == \"your\":\n",
    "            word = word.replace('your','')\n",
    "        if word == \"also\":\n",
    "            word = word.replace('also','')\n",
    "        if word == \"it's\":\n",
    "            word = word.replace(\"it's\",'')    \n",
    "        if word == \"only\":\n",
    "            word = word.replace('only','')\n",
    "        if word == \"there\":\n",
    "            word = word.replace('there','')\n",
    "        if word == \"their\":\n",
    "            word = word.replace('their','')\n",
    "        if word == \"get\":\n",
    "            word = word.replace('get','')\n",
    "        if word == \"other\":\n",
    "            word = word.replace('other','')    \n",
    "        if word == \"a\":\n",
    "            word = word.replace('a','')\n",
    "        if word == \"to\":\n",
    "            word = word.replace('to','')\n",
    "        if word == \"i\":\n",
    "            word = word.replace('i','')\n",
    "        if word == \"of\":\n",
    "            word = word.replace('of','')\n",
    "        if word == \"is\":\n",
    "            word = word.replace('is','')\n",
    "        if word == \"was\":\n",
    "            word = word.replace('was','')\n",
    "        if word == \"in\":\n",
    "            word = word.replace('in','')\n",
    "        if word == \"for\":\n",
    "            word = word.replace('for','')\n",
    "        if word == \"it\":\n",
    "            word = word.replace('it','')\n",
    "        if word == \"with\":\n",
    "            word = word.replace('with','')\n",
    "        if word == \"when\":\n",
    "            word = word.replace('when','')\n",
    "        if word == \"which\":\n",
    "            word = word.replace('which','')\n",
    "        if word == \"just\":\n",
    "            word = word.replace('just','')    \n",
    "        if word == \"you\":\n",
    "            word = word.replace('you','')\n",
    "        if word == \"we\":\n",
    "            word = word.replace('we','')\n",
    "        if word == \"but\":\n",
    "            word = word.replace('but','')            \n",
    "        if word == \"this\":\n",
    "            word = word.replace('this','')\n",
    "        if word == \"that\":\n",
    "            word = word.replace('that','')\n",
    "        if word == \"be\":\n",
    "            word = word.replace('be','')\n",
    "        if word == \"my\":\n",
    "            word = word.replace('my','')    \n",
    "        if word == \"have\":\n",
    "            word = word.replace('have','')\n",
    "        if word == \"if\":\n",
    "            word = word.replace('if','')\n",
    "        if word == \"at\":\n",
    "            word = word.replace('at','')    \n",
    "        if word == \"had\":\n",
    "            word = word.replace('had','')    \n",
    "        if word == \"and\":    \n",
    "            word = word.replace('and', '')\n",
    "        if word == \"come\":    \n",
    "            word = word.replace('come', '')\n",
    "        if word == \"got\":    \n",
    "            word = word.replace('got', '')    \n",
    "        if word == \"the\":        \n",
    "            word = word.replace('the', '')\n",
    "        if word == \"as\":        \n",
    "            word = word.replace('as', '')    \n",
    "        if word == \"they\":        \n",
    "            word = word.replace('they', '')\n",
    "        if word == \"are\":        \n",
    "            word = word.replace('are', '')\n",
    "        if word == \"on\":        \n",
    "            word = word.replace('on', '')\n",
    "        if word == \"so\":        \n",
    "            word = word.replace('so', '')\n",
    "        if word == \"were\":        \n",
    "            word = word.replace('were', '')    \n",
    "        if word == \"dumpling\":        \n",
    "            word = word.replace(\"dumpling\", \"dumplings\")\n",
    "        if word == \"dumplingss\":        \n",
    "            word = word.replace(\"dumplingss\", \"dumplings\")\n",
    "        if word != '':    \n",
    "            word_freq[word] += 1\n",
    "        #word_name.append(word)\n",
    "        #word_freq['test'] = word_freq['test'].append(word)\n",
    "        #word_name = word_name.append(word)\n",
    "        y = y+1\n",
    "        print('Word Counting:', y, '-------------')\n",
    "        print(word)\n",
    "        #word_name.append(word)\n",
    "\n",
    "word_freq_df = pd.DataFrame.from_dict(word_freq, orient='index') \\\n",
    ".sort_values(0, ascending=False) \\\n",
    ".rename(columns={0: 'Word_freq'}) \\\n",
    "\n",
    "word_freq_df.index.name = 'Word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp saving\n",
    "word_freq_df.to_csv (r'C:\\Users\\leroy\\Desktop\\seabassyelpmostrecom3.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "# Imports the methods needed to abstract classes into tables\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import MetaData\n",
    "Base = declarative_base()\n",
    "\n",
    "# Allow us to declare column types\n",
    "from sqlalchemy import Column, Integer, String, Float \n",
    "\n",
    "\n",
    "\n",
    "#Connect to local database\n",
    "rds_connection_string = \"postgres:2305nseW@localhost:5432/yelp_review_db\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas to load csv converted DataFrame into database\n",
    "#new_customer_data_df.to_sql(name='Yelp_reviews_seabass', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm data has been added by querying the customer_name table top 20 words mentioned\n",
    "\n",
    "new1_sb_shanghaijoe_db = pd.read_sql_query('select * from sb_top_reviews1', con=engine).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_sb_EatalyFlatiron_db = pd.read_sql_query('select * from sb_top_reviews2', con=engine).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new3_sb_IppudoWestside_db = pd.read_sql_query('select * from sb_top_reviews3', con=engine).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop common words to satify top 20 words relating to food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_drop = ['the', 'and', ]\n",
    "#new1_seabass_shanghaijoe_db[~new1_seabass_shanghaijoe_db['word'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1_sb_shanghaijoe_db['id'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1_sb_shanghaijoe_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new1_sb_shanghaijoe_db.astype({'id': 'int64'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_sb_EatalyFlatiron_db['id'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_sb_EatalyFlatiron_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new3_sb_IppudoWestside_db['id'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new3_sb_IppudoWestside_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_sb_EatalyFlatiron_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_sb_EatalyFlatiron_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new3_sb_IppudoWestside_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combing all 3 dataframes and placing unique Id on the word\n",
    "frames = [new1_sb_shanghaijoe_db, new2_sb_EatalyFlatiron_db, new3_sb_IppudoWestside_db]\n",
    "\n",
    "result = pd.concat(frames).reset_index(drop=True) #resetting the indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['id'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['id'] = result.groupby(['word']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toprevframes = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#switching varible\n",
    "toprevframes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a unique id column for word between top reviews 1st and 2nd\n",
    "zcounter = 0\n",
    "for z in range(0,len(result)):\n",
    "    for y in range(0,len(new2_sb_EatalyFlatiron_db)):\n",
    "        if result['word'][z] == new2_sb_EatalyFlatiron_db['word'][y]:\n",
    "            new2_sb_EatalyFlatiron_db['id'][y] = result['id'][z]\n",
    "            #print (\"found \" + str(new2_sb_EatalyFlatiron_db['id'][z]))\n",
    "        if result['word'][z] == new1_sb_shanghaijoe_db['word'][y]:\n",
    "            new1_sb_shanghaijoe_db['id'][y] = result['id'][z]    \n",
    "        if result['word'][z] == new3_sb_IppudoWestside_db['word'][y]:\n",
    "            new3_sb_IppudoWestside_db['id'][y] = result['id'][z]\n",
    "            #print (\"found \" + str(new3_sb_IppudoWestside_db['id'][z]))\n",
    "                       \n",
    "\n",
    "    for w in range(0,len(new1_sb_shanghaijoe_db)):        \n",
    "        if result['word'][z] == new2_sb_EatalyFlatiron_db['word'][w]:\n",
    "            new2_sb_EatalyFlatiron_db['id'][w] = result['id'][z]\n",
    "            #print (\"found \" + str(new2_sb_EatalyFlatiron_db['id'][z]))\n",
    "        if result['word'][z] == new1_sb_shanghaijoe_db['word'][w]:\n",
    "            new1_sb_shanghaijoe_db['id'][w] = result['id'][z]    \n",
    "        if result['word'][z] == new3_sb_IppudoWestside_db['word'][w]:\n",
    "            new3_sb_IppudoWestside_db['id'][w] = result['id'][z]\n",
    "            #print (\"found \" + str(new3_sb_IppudoWestside_db['id'][z]))\n",
    "            \n",
    "#     for y in range(0,len(new3_sb_IppudoWestside_db)):\n",
    "#         if result['word'][z] == new3_sb_IppudoWestside_db['word'][y]:\n",
    "#             new3_sb_IppudoWestside_db['id'][y] = result['id'][z]\n",
    "#             print (\"found \" + str(new3_sb_IppudoWestside_db['id'][z]))\n",
    "#             break\n",
    "        \n",
    "        \n",
    "        \n",
    "        #if new2_sb_EatalyFlatiron_db['word'][y] != new1_sb_shanghaijoe_db['word'][z]:\n",
    "        #    print (\"putting in new \" + str(y))\n",
    "            #new2_sb_EatalyFlatiron_db['id'][y] =  new2_sb_EatalyFlatiron_db['id'][y] + str(20 + zcounter)\n",
    "            #zcounter = zcounter + 1\n",
    "            #print (\"not found \" + str(new2_sb_EatalyFlatiron_db['id'][z]))\n",
    "        #else:\n",
    "        #    print (\"error\")\n",
    "   \n",
    "\n",
    "    \n",
    "#this code works    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_sb_EatalyFlatiron_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1_sb_shanghaijoe_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_sb_EatalyFlatiron_db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new3_sb_IppudoWestside_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting object to int for id column\n",
    "new1_sb_shanghaijoe_db[\"id\"] = new1_sb_shanghaijoe_db[\"id\"].astype(str).astype(int)\n",
    "\n",
    "#converting object to int for id column\n",
    "new2_sb_EatalyFlatiron_db[\"id\"] = new2_sb_EatalyFlatiron_db[\"id\"].astype(str).astype(int)\n",
    "\n",
    "#converting object to int for id column\n",
    "new3_sb_IppudoWestside_db[\"id\"] = new3_sb_IppudoWestside_db[\"id\"].astype(str).astype(int)\n",
    "\n",
    "print(new3_sb_IppudoWestside_db.dtypes)\n",
    "print(new2_sb_EatalyFlatiron_db.dtypes)\n",
    "print(new1_sb_shanghaijoe_db.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new3_sb_IppudoWestside_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing all the tables in the database\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query one of the tabel content\n",
    "\n",
    "\n",
    "sb_top_reviews1_db = pd.read_sql_query('select * from sb_top_reviews1', con=engine)\n",
    "sb_top_reviews1_db.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserting table into database\n",
    "# new2_sb_EatalyFlatiron_db.to_sql('new2_sb_EatalyFlatiron_db', engine)\n",
    "# new3_sb_IppudoWestside_db.to_sql('new3_sb_IppudoWestside_db', engine)\n",
    "# new1_sb_shanghaijoe_db.to_sql('new1_sb_shanghaijoe_db', engine)\n",
    "# toprevframes.to_sql('toprevframes', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"Metadata\" Layer That Abstracts our SQL Database\n",
    "# ----------------------------------\n",
    "#Base.metadata.create_all(engine)\n",
    "\n",
    "# Use this to clear out the db\n",
    "# ----------------------------------\n",
    "Base.metadata.drop_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing all the tables in the database\n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query one of the tabel content - Eataly\n",
    "sb_top_reviews2_db = pd.read_sql_query('select * from sb_top_reviews2', con=engine)\n",
    "sb_top_reviews2_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis\n",
    "## Compute sentiment score (positive score - negative score)\n",
    "## Return a sentiment polarity score: 1 = positive, 0 = negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_top_reviews1_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import positive file which contains common meaningless positive words such as good\n",
    "file_positive = open('positive.txt')\n",
    "reader =csv.reader(file_positive)\n",
    "positive_words = [word[0] for word in reader]\n",
    "\n",
    "## import negative file which contains common meaningless positive words such as bad\n",
    "file_negative = open('negative.txt')\n",
    "reader =csv.reader(file_negative)\n",
    "negative_words = [word[0] for word in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only keep positive and negative words\n",
    "def filter_words(review):\n",
    "    words = [word for word in review.split() if word in positive_words + negative_words]\n",
    "    words = ' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_top_reviews1_db['label'] =''\n",
    "sb_top_reviews1_db['Outcome'] =''\n",
    "sb_top_reviews1_db.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0\n",
    "for z in range(0,len(sb_top_reviews1_db)):\n",
    "    \n",
    "    for word in positive_words:\n",
    "        if word == sb_top_reviews1_db['word'][z]:\n",
    "            sb_top_reviews1_db['label'][z] ='positive'\n",
    "            sb_top_reviews1_db[\"Outcome\"][z] = 1\n",
    "            print (\"this is running-positive\" +str(y)+ word)\n",
    "            print (sb_top_reviews1_db['label'][z])\n",
    "            y =y +1\n",
    "            break;  \n",
    "    for negword in negative_words:        \n",
    "        if negword == sb_top_reviews1_db['word'][z]:\n",
    "            sb_top_reviews1_db['label'][z] ='negative'\n",
    "            sb_top_reviews1_db[\"Outcome\"][z] = 0\n",
    "            print (\"this is running-negative\")\n",
    "            y =y +1\n",
    "            break;\n",
    "    if sb_top_reviews1_db['label'][z] =='':\n",
    "            sb_top_reviews1_db['label'][z] ='neutral'            \n",
    "            sb_top_reviews1_db['Outcome'][z] = 2\n",
    "            print (\"this is running-neutral\" +str(y))\n",
    "            y =y +1\n",
    "             \n",
    "         \n",
    "            \n",
    "                \n",
    "z = z + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_top_reviews1_db['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_top_reviews1_db.head()\n",
    "sb_top_reviews1_db['label'].value_counts()\n",
    "\n",
    "\n",
    "top10pos1 = sb_top_reviews1_db[sb_top_reviews1_db['label'].str.match('positive')][:10]\n",
    "top10pos1\n",
    "\n",
    "top10neg1 = sb_top_reviews1_db[sb_top_reviews1_db['label'].str.match('negative')][:10]\n",
    "top10neg1.reset_index(drop=True, inplace=True)# resets the index\n",
    "top10neg1.index.name = 'Index'\n",
    "top10neg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [top10pos1, top10neg1]\n",
    "\n",
    "result_polar = pd.concat(frames)\n",
    "result_polar.reset_index(drop=True, inplace=True)\n",
    "result_polar.index.name = 'Index'\n",
    "result_polar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,6))\n",
    "colors = ['red' if c == 'negative' else 'blue' for c in result_polar.label]\n",
    "sns.barplot(y=result_polar.word, x=result_polar.word_freq, palette=colors)\n",
    "plt.xlabel('Word Frequency', labelpad=10, fontsize=14)\n",
    "plt.ylabel('Words', fontsize=14)\n",
    "plt.title('TOP 10 Positive & Negative Words in Shanghai Joe Restaurant Reviews', fontsize=15)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xticks(rotation=15)\n",
    "for  i, v in enumerate(result_polar.word_freq):\n",
    "    plt.text(v, i+0.25, str(v), fontweight='bold', color='green', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Fish cheek restaurant of the table content\n",
    "sb_yelp_mostrecomm3 = pd.read_sql_query('select * from sb_yelp_mostrecomm3', con=engine)\n",
    "sb_yelp_mostrecomm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_yelp_mostrecomm3.to_csv (r'C:\\Users\\leroy\\Desktop\\reviews7.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_yelp_mostrecomm3['label'] =''\n",
    "sb_yelp_mostrecomm3['Outcome'] =''\n",
    "sb_yelp_mostrecomm3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching positive words\n",
    "y = 0\n",
    "for z in range(0,len(sb_yelp_mostrecomm3)):\n",
    "    \n",
    "    for word in positive_words:\n",
    "        if word == sb_yelp_mostrecomm3['word'][z]:\n",
    "            sb_yelp_mostrecomm3['label'][z] ='positive'\n",
    "            sb_yelp_mostrecomm3[\"Outcome\"][z] = 1\n",
    "            print (\"this is running-positive\" +str(y)+ word)\n",
    "            print (sb_yelp_mostrecomm3['label'][z])\n",
    "            y =y +1\n",
    "            break;  \n",
    "\n",
    "         \n",
    "            \n",
    "                \n",
    "z = z + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching negative words\n",
    "y = 0\n",
    "for z in range(0,len(sb_yelp_mostrecomm3)):\n",
    "    for negword in negative_words:        \n",
    "        if negword == sb_yelp_mostrecomm3['word'][z]:\n",
    "            sb_yelp_mostrecomm3['label'][z] ='negative'\n",
    "            sb_yelp_mostrecomm3[\"Outcome\"][z] = 0\n",
    "            print (\"this is running-negative\"+str(y))\n",
    "            print (sb_yelp_mostrecomm3['label'][z])\n",
    "            y =y +1\n",
    "            break;\n",
    "#         elif sb_yelp_mostrecomm3['label'][z] =='':\n",
    "#             sb_yelp_mostrecomm3['label'][z] ='neutral'            \n",
    "#             sb_yelp_mostrecomm3['Outcome'][z] = 2\n",
    "#             print (\"this is running-neutral\" +str(y))\n",
    "#             print (sb_yelp_mostrecomm3['label'][z])\n",
    "#             y =y +1\n",
    "#             break; \n",
    "         \n",
    "            \n",
    "                \n",
    "z = z + 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matching neutral words\n",
    "y = 0\n",
    "for z in range(0,len(sb_yelp_mostrecomm3)):\n",
    "    if sb_yelp_mostrecomm3['label'][z] =='':\n",
    "            sb_yelp_mostrecomm3['label'][z] ='neutral'            \n",
    "            sb_yelp_mostrecomm3['Outcome'][z] = 2\n",
    "            print (\"this is running-neutral\" +str(y))\n",
    "            print (sb_yelp_mostrecomm3['label'][z])\n",
    "            y =y +1\n",
    "             \n",
    "         \n",
    "            \n",
    "                \n",
    "z = z + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_yelp_mostrecomm3.head()\n",
    "sb_yelp_mostrecomm3['label'].value_counts()\n",
    "\n",
    "\n",
    "top10pos1_mr = sb_yelp_mostrecomm3[sb_yelp_mostrecomm3['label'].str.match('positive')][:10]\n",
    "top10pos1_mr\n",
    "\n",
    "top10neg1_mr = sb_yelp_mostrecomm3[sb_yelp_mostrecomm3['label'].str.match('negative')][:10]\n",
    "#top10neg1_mr.reset_index(drop=True, inplace=True)# resets the index\n",
    "#top10neg1_mr.index.name = 'Index'\n",
    "top10neg1_mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_yelp_mostrecomm3['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for the sections of our pie chart\n",
    "labels = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "# The values of each section of the pie chart\n",
    "sizes = [257,157,2677]\n",
    "\n",
    "# The colors of each section of the pie chart\n",
    "colors = [\"lightskyblue\", \"red\", \"orange\"]\n",
    "\n",
    "# Tells matplotlib to seperate the \"Python\" section from the others\n",
    "explode = (0.1, 0, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the pie chart based upon the values above\n",
    "# Automatically finds the percentages of each part of the pie chart\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct=\"%1.1f%%\", shadow=True, startangle=140)\n",
    "plt.title(\"Shanghai Joe's\\n\" + \"overview\", bbox={'facecolor':'0.8', 'pad':5})\n",
    "plt.xlabel('User Reviews')\n",
    "plt.ylabel('Words')\n",
    "#plt.title('Interesting Graph\\nCheck it out')\n",
    "plt.legend()\n",
    "plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames2 = [top10pos1_mr, top10neg1_mr]\n",
    "\n",
    "result_polar2 = pd.concat(frames2)\n",
    "result_polar2.reset_index(drop=True, inplace=True)\n",
    "result_polar2.index.name = 'Index'\n",
    "result_polar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,6))\n",
    "colors = ['red' if c == 'negative' else 'blue' for c in result_polar2.label]\n",
    "sns.barplot(y=result_polar2.word, x=result_polar2.word_freq, palette=colors)\n",
    "plt.xlabel('Word Frequency', labelpad=10, fontsize=14)\n",
    "plt.ylabel('Words', fontsize=14)\n",
    "plt.title('TOP 10 Positive & Negative Words in Fish Cheeks Reviews', fontsize=15)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xticks(rotation=15)\n",
    "for  i, v in enumerate(result_polar2.word_freq):\n",
    "    plt.text(v, i+0.3, str(v), fontweight='bold', color='green', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_top_reviews1_db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Hypothesis**: Can we predict the time from pour using the measurements of foam height and beer height?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables/Columns TIME: Time from pour (seconds) 4-8 FOAM: Wet foam height (cm) 10-16 BEER: Beer height (cm) 18-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv into dataframe\n",
    "\n",
    "reviews_ml = pd.read_csv('reviews_ml.csv', encoding='latin-1') # Yelp folder\n",
    "reviews_ml = reviews_ml.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>label</th>\n",
       "      <th>reviews</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shanghai_joe</td>\n",
       "      <td>mostreviewed1</td>\n",
       "      <td>Made the trip to Chinatown to pick-up from Joe...</td>\n",
       "      <td>Seafood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Eataly</td>\n",
       "      <td>mostreviewed2</td>\n",
       "      <td>I have been going to this place to eat at thei...</td>\n",
       "      <td>Italian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ippudo_ramen</td>\n",
       "      <td>mostreviewed3</td>\n",
       "      <td>I love this style of Ramen - such a satisfying...</td>\n",
       "      <td>Ramen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>fish_cheeks</td>\n",
       "      <td>mostrecommended1</td>\n",
       "      <td>We went here during Restaurant Week 2020. I ha...</td>\n",
       "      <td>Seafood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>kotobuki</td>\n",
       "      <td>mostrecommended2</td>\n",
       "      <td>FOODCrispy rice spicy tuna app was delicious! ...</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Au_zur</td>\n",
       "      <td>mostrecommended3</td>\n",
       "      <td>Delicious flavored food; great concept to show...</td>\n",
       "      <td>Wine bar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    restaurant             label  \\\n",
       "0      1  Shanghai_joe     mostreviewed1   \n",
       "1      2        Eataly     mostreviewed2   \n",
       "2      3  ippudo_ramen     mostreviewed3   \n",
       "3      4   fish_cheeks  mostrecommended1   \n",
       "4      5      kotobuki  mostrecommended2   \n",
       "5      6        Au_zur  mostrecommended3   \n",
       "\n",
       "                                             reviews  category  \n",
       "0  Made the trip to Chinatown to pick-up from Joe...   Seafood  \n",
       "1  I have been going to this place to eat at thei...   Italian  \n",
       "2  I love this style of Ramen - such a satisfying...     Ramen  \n",
       "3  We went here during Restaurant Week 2020. I ha...   Seafood  \n",
       "4  FOODCrispy rice spicy tuna app was delicious! ...  Japanese  \n",
       "5  Delicious flavored food; great concept to show...  Wine bar  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods to clean up articles\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "stop_words_ = set(stopwords.words('english'))\n",
    "wn = WordNetLemmatizer()\n",
    "def accept_words(token):\n",
    "    return token not in stop_words_ and token not in list(string.punctuation)\n",
    "def clean_txt(text):\n",
    "    clean_text = []\n",
    "    text = re.sub(\"'\", \"\",text)\n",
    "    text=re.sub(\"[\\d\\W]+\",\" \",text)  \n",
    "    clean_text = [ wn.lemmatize(word) for word in word_tokenize(text.lower()) if accept_words(word)]\n",
    "    return \" \".join(clean_text)\n",
    "\n",
    "# additional features (subjectivity and polarity)\n",
    "# however, have not used polarity since naive bayes can't take negative values\n",
    "from textblob import TextBlob\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "import textstat\n",
    "\n",
    "def hard_words(text):\n",
    "    total_words = len(text.split())\n",
    "    return textstat.difficult_words(text)/total_words\n",
    "def subj_txt(text):\n",
    "    return TextBlob(text).sentiment[1]\n",
    "#def polarity_txt(text):\n",
    " #   return (TextBlob(text).sentiment[0] + 1)/2\n",
    "def readability(text):\n",
    "    return textstat.automated_readability_index(text)\n",
    "def unique_words(text):\n",
    "    return len(set(clean_txt(text).split()))/ len(text.split())\n",
    "\n",
    "\n",
    "\n",
    "reviews_ml['subj'] = reviews_ml['reviews'].apply(subj_txt) # personal opinion, emotion or judgment whereas objective refers to factual information\n",
    "#reviews_ml['pol'] = reviews_ml['reviews'].apply(polarity_txt)\n",
    "reviews_ml['difficult_words'] = reviews_ml['reviews'].apply(hard_words) \n",
    "reviews_ml['readability'] = reviews_ml['reviews'].apply(readability)\n",
    "reviews_ml['unique_words'] = reviews_ml['reviews'].apply(unique_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>label</th>\n",
       "      <th>reviews</th>\n",
       "      <th>category</th>\n",
       "      <th>subj</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>readability</th>\n",
       "      <th>unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shanghai_joe</td>\n",
       "      <td>mostreviewed1</td>\n",
       "      <td>Made the trip to Chinatown to pick-up from Joe...</td>\n",
       "      <td>Seafood</td>\n",
       "      <td>0.533423</td>\n",
       "      <td>0.077656</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.213048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Eataly</td>\n",
       "      <td>mostreviewed2</td>\n",
       "      <td>I have been going to this place to eat at thei...</td>\n",
       "      <td>Italian</td>\n",
       "      <td>0.544122</td>\n",
       "      <td>0.105055</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.268167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ippudo_ramen</td>\n",
       "      <td>mostreviewed3</td>\n",
       "      <td>I love this style of Ramen - such a satisfying...</td>\n",
       "      <td>Ramen</td>\n",
       "      <td>0.588851</td>\n",
       "      <td>0.085482</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.229308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>fish_cheeks</td>\n",
       "      <td>mostrecommended1</td>\n",
       "      <td>We went here during Restaurant Week 2020. I ha...</td>\n",
       "      <td>Seafood</td>\n",
       "      <td>0.593936</td>\n",
       "      <td>0.080390</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.221072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>kotobuki</td>\n",
       "      <td>mostrecommended2</td>\n",
       "      <td>FOODCrispy rice spicy tuna app was delicious! ...</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>0.555991</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.266639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Au_zur</td>\n",
       "      <td>mostrecommended3</td>\n",
       "      <td>Delicious flavored food; great concept to show...</td>\n",
       "      <td>Wine bar</td>\n",
       "      <td>0.597379</td>\n",
       "      <td>0.110359</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.281673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    restaurant             label  \\\n",
       "0      1  Shanghai_joe     mostreviewed1   \n",
       "1      2        Eataly     mostreviewed2   \n",
       "2      3  ippudo_ramen     mostreviewed3   \n",
       "3      4   fish_cheeks  mostrecommended1   \n",
       "4      5      kotobuki  mostrecommended2   \n",
       "5      6        Au_zur  mostrecommended3   \n",
       "\n",
       "                                             reviews  category      subj  \\\n",
       "0  Made the trip to Chinatown to pick-up from Joe...   Seafood  0.533423   \n",
       "1  I have been going to this place to eat at thei...   Italian  0.544122   \n",
       "2  I love this style of Ramen - such a satisfying...     Ramen  0.588851   \n",
       "3  We went here during Restaurant Week 2020. I ha...   Seafood  0.593936   \n",
       "4  FOODCrispy rice spicy tuna app was delicious! ...  Japanese  0.555991   \n",
       "5  Delicious flavored food; great concept to show...  Wine bar  0.597379   \n",
       "\n",
       "   difficult_words  readability  unique_words  \n",
       "0         0.077656          8.0      0.213048  \n",
       "1         0.105055          8.9      0.268167  \n",
       "2         0.085482          7.0      0.229308  \n",
       "3         0.080390          7.1      0.221072  \n",
       "4         0.097561          7.5      0.266639  \n",
       "5         0.110359          7.0      0.281673  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep dive into Seafood Vs other protein Groups ??? - TBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running search through yelp and return the max results\n",
    "data = []\n",
    "count = 0\n",
    "for offset in range(0, 1000, 50):\n",
    "        params = {\n",
    "            'limit': 50, \n",
    "            'location':'New York',\n",
    "            'term': 'steak',\n",
    "            'offset': offset\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data += response.json()['businesses']\n",
    "            count = count + 1\n",
    "        elif response.status_code == 400:\n",
    "            print('400 Bad Request')\n",
    "            break\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the results of the search in dataframe\n",
    "df2 = pd.DataFrame(data)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['new_url2', 'new_url3']]=df2['url'].str.split('?', expand=True,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['new_url2'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_luger = df2['new_url2'][0]\n",
    "club_a = df2['new_url2'][1]\n",
    "rocco = df2['new_url2'][2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seabass\n",
    "#url_top1\n",
    "#url_top2\n",
    "#url_top3\n",
    "#url_rec1\n",
    "#url_rec2\n",
    "#url_rec3\n",
    "\n",
    "\n",
    "start = 0\n",
    "num_pages = 7\n",
    "end = 20* num_pages\n",
    "x = 1\n",
    "reviews = []\n",
    "quote_add = []\n",
    "\n",
    "while (start < end):\n",
    "    try:\n",
    "        new_url = peter_luger + '?start=' + str(start)\n",
    "        start +=20\n",
    "        print (new_url)\n",
    "        response = requests.get(new_url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    #print(soup.prettify())\n",
    "    #instructions = recipeSoup.find('div', class_='hidden').text\n",
    "    #right_table4=soup.find_all('p', itemprop='description')\n",
    "        quotes = soup.find_all('span', class_ = 'lemon--span__373c0__3997G raw__373c0__3rKqk')\n",
    "        for quote in quotes:\n",
    "            x = x+1\n",
    "            print('Review:', x, '-------------')\n",
    "            print(quote.text)\n",
    "            quote_add.append(quote.text)\n",
    "#-------------------------------------------------------------------newly added\n",
    "#Makes all the letters lower in reviews\n",
    "            review = quote.text.lower()\n",
    "#expandContractions will put the dictionary made earlier to replace the contractions in the reviews\n",
    "#Make sure to to run the cList dict cell or else there will be an error\n",
    "            expandContractions(review)\n",
    "#Cleaning the lemmas or words in reviews now will make it easier when we start predictive modeling\n",
    "            words = word_tokenize(review)\n",
    "            words = word_tokenize(review.replace('\\n',' '))\n",
    "            clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "            characters_to_remove = [\"''\",'``','...']\n",
    "            clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "            english_stops = set(stopwords.words('english'))\n",
    "            clean_words = [word for word in clean_words if word not in english_stops]\n",
    "            wordnet_lemmatizer = WordNetLemmatizer()\n",
    "            lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "            #lemma_list = str.replace(right_table4.get_text(strip=True),\". \",\".\")\n",
    "            reviews.append(lemma_list)\n",
    "    #reviews.add(a)\n",
    "    except:\n",
    "    #reviews.append(right_table4)\n",
    "        time.sleep(5)\n",
    "    #print(\"Reviews:\", a,\"\\n\")\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_l = pd.DataFrame(quote_add, columns=['reviews'])\n",
    "# reviews_collect_cleaned = pd.DataFrame(np.array(reviews), columns=['review'])\n",
    "# reviews_collect_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peter_l.to_csv (r'C:\\Users\\leroy\\Desktop\\reviews8.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_l.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_l['subj'] = peter_l['reviews'].apply(subj_txt)\n",
    "peter_l['pol'] = peter_l['reviews'].apply(polarity_txt)\n",
    "peter_l['difficult_words'] = peter_l['reviews'].apply(hard_words)\n",
    "peter_l['readability'] = peter_l['reviews'].apply(readability)\n",
    "peter_l['unique_words'] = peter_l['reviews'].apply(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_l = peter_l.drop([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_l = peter_l.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peter_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
